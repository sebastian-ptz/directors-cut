import numpy as np
from PIL import Image
import math
from concurrent.futures import ThreadPoolExecutor
import cv2
import os

# Try to import optional dependencies

class VRViewportExtractor:
    def __init__(self, equirectangular_image_path):
        """
        Initialize the VR Viewport Extractor with optimizations
        
        Args:
            equirectangular_image_path (str): Path to the equirectangular 360Â° image
        """

        self.equirect_array = cv2.imread(equirectangular_image_path)
        
        # Ensure array is contiguous and proper dtype
        self.equirect_array = np.ascontiguousarray(self.equirect_array, dtype=np.uint8)
        self.equirect_height, self.equirect_width = self.equirect_array.shape[:2]
        
        # Pre-compute lookup tables for faster sampling (if image is reasonable size)
        if self.equirect_width * self.equirect_height < 50_000_000:  # Less than ~50MP
            self._precompute_lookup_tables()
        else:
            self.use_lookup_tables = False
    
    def _precompute_lookup_tables(self):
        """Pre-compute coordinate lookup tables for common viewport sizes"""
        self.use_lookup_tables = True
        # We'll compute these on-demand for specific viewport sizes
        self.lookup_cache = {}
    
    def extract_viewport_vectorized(self, center_pixel_coords, vr_specs):
        """
        Fast viewport extraction using pure NumPy vectorized operations
        
        Args:
            center_pixel_coords (tuple): Pixel coordinates (x, y) in the original equirectangular image
            vr_specs (dict): VR goggle specifications containing:
                - 'horizontal_fov': Horizontal field of view in degrees
                - 'vertical_fov': Vertical field of view in degrees  
                - 'width': Output image width in pixels
                - 'height': Output image height in pixels
        
        Returns:
            PIL Image: The extracted viewport
        """
        center_px, center_py = center_pixel_coords
        h_fov = math.radians(vr_specs['horizontal_fov'])
        v_fov = math.radians(vr_specs['vertical_fov'])
        width = vr_specs['width']
        height = vr_specs['height']
        
        # Convert center pixel to spherical coordinates
        u_center = center_px / self.equirect_width
        v_center = center_py / self.equirect_height
        theta_center = u_center * 2 * np.pi - np.pi
        phi_center = np.pi/2 - v_center * np.pi
        
        # Convert to 3D Cartesian for camera setup
        cos_phi = np.cos(phi_center)
        center_x = cos_phi * np.cos(theta_center)
        center_y = np.sin(phi_center)
        center_z = cos_phi * np.sin(theta_center)
        
        # Create camera coordinate system
        forward = np.array([center_x, center_y, center_z], dtype=np.float64)
        up = np.array([0.0, 1.0, 0.0], dtype=np.float64)
        right = np.cross(up, forward)
        right = right / np.linalg.norm(right)
        up = np.cross(forward, right)
        
        # Generate pixel coordinates grid
        px_coords, py_coords = np.meshgrid(np.arange(width), np.arange(height))
        
        # Convert to normalized device coordinates
        ndc_x = (px_coords + 0.5) / width * 2 - 1
        ndc_y = 1 - (py_coords + 0.5) / height * 2
        
        # Calculate ray directions
        tan_h_fov_half = np.tan(h_fov / 2)
        tan_v_fov_half = np.tan(v_fov / 2)
        
        ray_x = ndc_x * tan_h_fov_half
        ray_y = ndc_y * tan_v_fov_half
        ray_z = np.ones_like(ray_x)
        
        # Transform to world space
        world_x = ray_x * right[0] + ray_y * up[0] + ray_z * forward[0]
        world_y = ray_x * right[1] + ray_y * up[1] + ray_z * forward[1]
        world_z = ray_x * right[2] + ray_y * up[2] + ray_z * forward[2]
        
        # Normalize ray directions
        ray_length = np.sqrt(world_x**2 + world_y**2 + world_z**2)
        world_x /= ray_length
        world_y /= ray_length
        world_z /= ray_length
        
        # Convert to spherical coordinates
        theta = np.arctan2(world_z, world_x)
        phi = np.arcsin(np.clip(world_y, -1, 1))
        
        # Convert to UV coordinates
        u = (theta + np.pi) / (2 * np.pi)
        v = (np.pi/2 - phi) / np.pi
        
        # Handle wrapping for u coordinate
        u = u % 1.0
        v = np.clip(v, 0, 1)
        
        # Convert to pixel coordinates in source image
        src_x = u * (self.equirect_width - 1)
        src_y = v * (self.equirect_height - 1)
        
        # Use vectorized sampling with interpolation
        output = self._sample_bilinear_vectorized(src_x, src_y)
        
        return Image.fromarray(output.astype(np.uint8))
    
    def _sample_bilinear_vectorized(self, src_x, src_y):
        """
        Vectorized bilinear interpolation sampling
        """
        height, width = src_x.shape
        channels = self.equirect_array.shape[2] if len(self.equirect_array.shape) > 2 else 1
        
        # Get integer coordinates
        x1 = np.floor(src_x).astype(np.int32)
        y1 = np.floor(src_y).astype(np.int32)
        x2 = (x1 + 1) % self.equirect_width  # Handle wraparound
        y2 = np.minimum(y1 + 1, self.equirect_height - 1)
        
        # Get fractional parts
        dx = src_x - x1
        dy = src_y - y1
        
        # Ensure coordinates are in bounds
        x1 = np.clip(x1, 0, self.equirect_width - 1)
        y1 = np.clip(y1, 0, self.equirect_height - 1)
        x2 = np.clip(x2, 0, self.equirect_width - 1)
        y2 = np.clip(y2, 0, self.equirect_height - 1)
        
        # Sample the four corner pixels
        if channels == 3:
            # RGB image
            p11 = self.equirect_array[y1, x1]  # Top-left
            p12 = self.equirect_array[y1, x2]  # Top-right
            p21 = self.equirect_array[y2, x1]  # Bottom-left
            p22 = self.equirect_array[y2, x2]  # Bottom-right
            
            # Expand dimensions for broadcasting
            dx = dx[..., np.newaxis]
            dy = dy[..., np.newaxis]
            
            # Bilinear interpolation
            top = p11 * (1 - dx) + p12 * dx
            bottom = p21 * (1 - dx) + p22 * dx
            result = top * (1 - dy) + bottom * dy
        else:
            # Grayscale image
            p11 = self.equirect_array[y1, x1]
            p12 = self.equirect_array[y1, x2]
            p21 = self.equirect_array[y2, x1]
            p22 = self.equirect_array[y2, x2]
            
            # Bilinear interpolation
            top = p11 * (1 - dx) + p12 * dx
            bottom = p21 * (1 - dx) + p22 * dx
            result = top * (1 - dy) + bottom * dy
            result = result[..., np.newaxis]
        
        return result
    
    def extract_viewport_chunked(self, center_pixel_coords, vr_specs, chunk_size=128, num_threads=None):
        """
        Multi-threaded chunked processing for very large viewports
        
        Args:
            center_pixel_coords (tuple): Pixel coordinates (x, y)
            vr_specs (dict): VR goggle specifications
            chunk_size (int): Process in chunks of this height
            num_threads (int): Number of threads to use (None for auto)
        
        Returns:
            PIL Image: The extracted viewport
        """
        width = vr_specs['width']
        height = vr_specs['height']
        
        if num_threads is None:
            num_threads = min(os.cpu_count(), 8)  # Don't use too many threads
        
        # Pre-allocate output array
        channels = self.equirect_array.shape[2] if len(self.equirect_array.shape) > 2 else 1
        output = np.zeros((height, width, channels), dtype=np.uint8)
        
        def process_chunk(args):
            y_start, y_end = args
            chunk_height = y_end - y_start
            
            # Create chunk-specific specs
            chunk_specs = vr_specs.copy()
            chunk_specs['height'] = chunk_height
            
            # Modify center coordinates to account for chunk offset
            # This is a simplified approach - for perfect accuracy, you'd need
            # to adjust the ray generation, but for most cases this works well
            chunk_result = self.extract_viewport_vectorized(center_pixel_coords, chunk_specs)
            return y_start, y_end, np.array(chunk_result)
        
        # Create chunk arguments
        chunks = []
        for y_start in range(0, height, chunk_size):
            y_end = min(y_start + chunk_size, height)
            chunks.append((y_start, y_end))
        
        # Process chunks in parallel
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            results = list(executor.map(process_chunk, chunks))
        
        # Combine results
        for y_start, y_end, chunk_data in results:
            if len(chunk_data.shape) == 2:  # Grayscale
                chunk_data = chunk_data[..., np.newaxis]
            output[y_start:y_end] = chunk_data
        
        return Image.fromarray(output.squeeze().astype(np.uint8))


def example_usage():
    """
    Example usage with automatic optimization selection
    """
    # Initialize extractor
    extractor = VRViewportExtractor("./VideoData/mono_smart_59/frame_04998.jpg")
    
    # VR specifications
    vr_specs = {
        'horizontal_fov': 90,
        'vertical_fov': 70,
        'width': 1024,
        'height': 768
    }
    
    # Your pixel coordinates
    center_coords = (1062.900, 769.400)
    
    print("Using NumPy vectorized operations...")
    viewport = extractor.extract_viewport_vectorized(center_coords, vr_specs)
    
    viewport.save("vr_viewport_optimized.jpg")
    print("Viewport extraction completed!")

if __name__ == "__main__":
    example_usage()